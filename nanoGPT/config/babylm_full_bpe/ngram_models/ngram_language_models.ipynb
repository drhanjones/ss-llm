{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.util import bigrams, trigrams\n",
    "from nltk.lm import MLE, Laplace\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 8000 (inside /home/abishekthamma/PycharmProjects/masters_thesis/ss-llm/nanoGPT/data/babylm_full_bpe_8k/meta.pkl)\n"
     ]
    }
   ],
   "source": [
    "dataset_root = r'/home/abishekthamma/PycharmProjects/masters_thesis/ss-llm/nanoGPT/data'\n",
    "dataset_name = 'babylm_full_bpe_8k'\n",
    "\n",
    "data_dir = os.path.join(dataset_root, dataset_name)\n",
    "\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta.get('vocab_size', None)\n",
    "    if meta_vocab_size:\n",
    "        print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m train_bigrams \u001B[38;5;241m=\u001B[39m bigrams(train_data)\n\u001B[0;32m----> 2\u001B[0m train_bigram_counts \u001B[38;5;241m=\u001B[39m \u001B[43mCounter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_bigrams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m val_bigrams \u001B[38;5;241m=\u001B[39m bigrams(val_data)\n\u001B[1;32m      4\u001B[0m val_bigrams_counts \u001B[38;5;241m=\u001B[39m Counter(val_bigrams)\n",
      "File \u001B[0;32m/usr/lib/python3.9/collections/__init__.py:593\u001B[0m, in \u001B[0;36mCounter.__init__\u001B[0;34m(self, iterable, **kwds)\u001B[0m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m'''Create a new, empty Counter object.  And if given, count elements\u001B[39;00m\n\u001B[1;32m    583\u001B[0m \u001B[38;5;124;03mfrom an input iterable.  Or, initialize the count from another mapping\u001B[39;00m\n\u001B[1;32m    584\u001B[0m \u001B[38;5;124;03mof elements to their counts.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    590\u001B[0m \n\u001B[1;32m    591\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m    592\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m--> 593\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.9/collections/__init__.py:679\u001B[0m, in \u001B[0;36mCounter.update\u001B[0;34m(self, iterable, **kwds)\u001B[0m\n\u001B[1;32m    677\u001B[0m             \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mupdate(iterable)\n\u001B[1;32m    678\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 679\u001B[0m         \u001B[43m_count_elements\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    680\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwds:\n\u001B[1;32m    681\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate(kwds)\n",
      "File \u001B[0;32m~/PycharmProjects/masters_thesis/mt1/lib/python3.9/site-packages/nltk/util.py:887\u001B[0m, in \u001B[0;36mbigrams\u001B[0;34m(sequence, **kwargs)\u001B[0m\n\u001B[1;32m    871\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbigrams\u001B[39m(sequence, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    872\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    873\u001B[0m \u001B[38;5;124;03m    Return the bigrams generated from a sequence of items, as an iterator.\u001B[39;00m\n\u001B[1;32m    874\u001B[0m \u001B[38;5;124;03m    For example:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    884\u001B[0m \u001B[38;5;124;03m    :rtype: iter(tuple)\u001B[39;00m\n\u001B[1;32m    885\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 887\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m ngrams(sequence, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/PycharmProjects/masters_thesis/mt1/lib/python3.9/site-packages/numpy/core/memmap.py:334\u001B[0m, in \u001B[0;36mmemmap.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    331\u001B[0m     \u001B[38;5;66;03m# Return ndarray otherwise\u001B[39;00m\n\u001B[1;32m    332\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39mndarray)\n\u001B[0;32m--> 334\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, index):\n\u001B[1;32m    335\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getitem__\u001B[39m(index)\n\u001B[1;32m    336\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(res) \u001B[38;5;129;01mis\u001B[39;00m memmap \u001B[38;5;129;01mand\u001B[39;00m res\u001B[38;5;241m.\u001B[39m_mmap \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_bigrams = bigrams(train_data)\n",
    "train_bigram_counts = Counter(train_bigrams)\n",
    "val_bigrams = bigrams(val_data)\n",
    "val_bigrams_counts = Counter(val_bigrams)\n",
    "train_trigrams = trigrams(train_data)\n",
    "train_trigram_counts = Counter(train_trigrams)\n",
    "val_trigrams = trigrams(val_data)\n",
    "val_trigrams_counts = Counter(val_trigrams)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "[((14, 133), 98191),\n ((202, 173), 50943),\n ((12, 207), 45365),\n ((203, 173), 44553),\n ((14, 47), 37822)]"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bigram_counts.most_common(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def prepare_bigram_probs(bigrams_counts, vocab_size):\n",
    "    bigram_arr = torch.zeros((vocab_size, vocab_size), dtype=torch.int32)\n",
    "    for k,v in bigrams_counts.items():\n",
    "        bigram_arr[k[1], k[0]] = v\n",
    "\n",
    "    bigram_prob = (bigram_arr+1).float()\n",
    "    bigram_probs = bigram_prob /bigram_prob.sum(1, keepdim=True)\n",
    "\n",
    "    return bigram_probs\n",
    "\n",
    "def prepare_trigram_probs(trigrams, bigrams, vocab_size):\n",
    "\n",
    "    trigram_counts = Counter(trigrams)\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    trigram_probs = {}\n",
    "\n",
    "    for k,v in tqdm(trigram_counts.items()):\n",
    "        trigram_probs[k] = float(v+1)/(bigram_counts[(k[0], k[1])]+vocab_size)\n",
    "\n",
    "    return trigram_probs\n",
    "\n",
    "\n",
    "\n",
    "train_bigram_probs = prepare_bigram_probs(train_bigram_counts, meta_vocab_size)\n",
    "val_bigram_probs = prepare_bigram_probs(val_bigrams_counts, meta_vocab_size)\n",
    "\n",
    "train_trigram_probs = prepare_trigram_probs(trigrams(train_data), bigrams(train_data), meta_vocab_size)\n",
    "val_trigram_probs = prepare_trigram_probs(trigrams(val_data), bigrams(val_data), meta_vocab_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_bigram_arr = torch.zeros((meta_vocab_size, meta_vocab_size), dtype=torch.int32)\n",
    "# val_bigram_arr = torch.zeros((meta_vocab_size, meta_vocab_size), dtype=torch.int32)\n",
    "# for k,v in train_bigram_counts.items():\n",
    "#     train_bigram_arr[k[0], k[1]] = v\n",
    "# for k,v in val_bigrams_counts.items():\n",
    "#     val_bigram_arr[k[0], k[1]] = v\n",
    "# train_bigram_probs = (train_bigram_arr+1).float()\n",
    "# train_probs = train_bigram_probs /train_bigram_probs.sum(1, keepdim=True)\n",
    "# val_bigram_probs = (val_bigram_arr+1).float()\n",
    "# val_probs = val_bigram_probs/val_bigram_probs.sum(1, keepdim=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def calculate_bigram_loss(probs, corpus):\n",
    "    loss = 0\n",
    "    counts = 0\n",
    "    for i in tqdm(range(1, len(corpus))):\n",
    "        loss += -torch.log(probs[corpus[i], corpus[i-1]])\n",
    "        counts += 1\n",
    "    return loss/counts\n",
    "\n",
    "def calculate_trigram_loss(probs, corpus, vocab_size):\n",
    "    loss = 0\n",
    "    counts = 0\n",
    "    for i in tqdm(range(2, len(corpus))):\n",
    "        trigram_probs = probs.get((corpus[i-2], corpus[i-1], corpus[i]), 1/vocab_size)\n",
    "        loss += -math.log(trigram_probs)\n",
    "        counts += 1\n",
    "    return loss/counts\n",
    "\n",
    "def calculate_loss(probabilities, corpus, loss_type = 'bigram', vocab_size = None):\n",
    "\n",
    "    if loss_type == 'bigram':\n",
    "        return calculate_bigram_loss(probabilities, corpus)\n",
    "    elif loss_type == 'trigram':\n",
    "        if not vocab_size:\n",
    "            raise Exception\n",
    "        return calculate_trigram_loss(probabilities, corpus, vocab_size)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid loss type {loss_type}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14545184/14545184 [02:32<00:00, 95382.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram_loss = 5.702608585357666 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_bigrams_loss = calculate_loss(train_bigram_probs, train_data, loss_type=\"bigram\")\n",
    "print(f\"bigram_train_loss = {train_bigrams_loss} \")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13921893/13921893 [02:27<00:00, 94094.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram_val_loss = 5.8382182121276855 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14545183/14545183 [00:26<00:00, 549868.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram train _loss = 6.911967814899822 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13921892/13921892 [00:25<00:00, 538980.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram_val_loss = 7.324238471596719 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_bigrams_loss = calculate_loss(train_bigram_probs, val_data, loss_type=\"bigram\")\n",
    "print(f\"bigram_val_loss = {val_bigrams_loss} \")\n",
    "\n",
    "train_trigrams_loss = calculate_loss(train_trigram_probs, train_data, loss_type=\"trigram\", vocab_size=meta_vocab_size)\n",
    "print(f\"trigram train _loss = {train_trigrams_loss} \")\n",
    "\n",
    "train_bigrams_loss = calculate_loss(train_trigram_probs, val_data, loss_type=\"trigram\", vocab_size=meta_vocab_size)\n",
    "print(f\"trigram_val_loss = {train_bigrams_loss} \")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot probability distribution of 1 word\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_bigram_probs[7220].numpy())\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 923/923 [00:00<00:00, 625014.95it/s]\n",
      "100%|██████████| 999/999 [00:00<00:00, 65592.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram_test_loss = 5.205868244171143 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [00:00<00:00, 890620.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram_test_loss = 5.34031715983772 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#convert dataset to list of strings\n",
    "testing_subset = train_data[:1000]\n",
    "ts_vocab_size = len(set(testing_subset))\n",
    "\n",
    "#map values as 0 to vocab_size\n",
    "itos_mapper = {}\n",
    "for i in testing_subset:\n",
    "    itos_mapper[i] = itos_mapper.get(i, len(itos_mapper))\n",
    "\n",
    "testing_subset = [itos_mapper[i] for i in testing_subset]\n",
    "\n",
    "\n",
    "ts_bigrams = bigrams(testing_subset)\n",
    "ts_bigram_counts = Counter(ts_bigrams)\n",
    "ts_trigrams = trigrams(testing_subset)\n",
    "ts_trigram_counts = Counter(ts_trigrams)\n",
    "\n",
    "ts_bigram_probs = prepare_bigram_probs(ts_bigram_counts, ts_vocab_size)\n",
    "ts_trigram_probs = prepare_trigram_probs(trigrams(testing_subset), bigrams(testing_subset), ts_vocab_size)\n",
    "\n",
    "ts_bigram_loss = calculate_loss(ts_bigram_probs, testing_subset, loss_type=\"bigram\")\n",
    "print(f\"bigram_test_loss = {ts_bigram_loss} \")\n",
    "\n",
    "ts_trigram_loss = calculate_loss(ts_trigram_probs, testing_subset, loss_type=\"trigram\", vocab_size=ts_vocab_size)\n",
    "print(f\"trigram_test_loss = {ts_trigram_loss} \")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[0,\n 1,\n 2,\n 3,\n 4,\n 5,\n 6,\n 7,\n 8,\n 9,\n 10,\n 11,\n 12,\n 13,\n 14,\n 15,\n 16,\n 17,\n 18,\n 19,\n 20,\n 2,\n 21,\n 15,\n 22,\n 23,\n 24,\n 25,\n 26,\n 27,\n 28,\n 29,\n 30,\n 16,\n 31,\n 17,\n 32,\n 33,\n 34,\n 35,\n 36,\n 37,\n 38,\n 25,\n 39,\n 40,\n 41,\n 42,\n 43,\n 22,\n 44,\n 15,\n 16,\n 45,\n 31,\n 46,\n 47,\n 48,\n 15,\n 22,\n 49,\n 8,\n 16,\n 50,\n 51,\n 52,\n 53,\n 54,\n 55,\n 56,\n 57,\n 22,\n 25,\n 58,\n 59,\n 3,\n 60,\n 61,\n 62,\n 63,\n 30,\n 16,\n 17,\n 64,\n 65,\n 66,\n 67,\n 47,\n 25,\n 68,\n 69,\n 70,\n 30,\n 71,\n 3,\n 72,\n 73,\n 22,\n 74,\n 75,\n 8,\n 76,\n 77,\n 15,\n 78,\n 79,\n 78,\n 79,\n 80,\n 81,\n 82,\n 83,\n 84,\n 85,\n 86,\n 87,\n 88,\n 89,\n 69,\n 90,\n 91,\n 92,\n 93,\n 94,\n 69,\n 90,\n 91,\n 92,\n 8,\n 31,\n 95,\n 23,\n 25,\n 88,\n 96,\n 97,\n 69,\n 23,\n 98,\n 99,\n 72,\n 100,\n 94,\n 69,\n 23,\n 13,\n 101,\n 102,\n 15,\n 103,\n 72,\n 23,\n 104,\n 94,\n 105,\n 106,\n 107,\n 98,\n 69,\n 23,\n 108,\n 109,\n 110,\n 111,\n 112,\n 15,\n 113,\n 114,\n 115,\n 116,\n 8,\n 23,\n 117,\n 118,\n 8,\n 72,\n 23,\n 104,\n 118,\n 119,\n 22,\n 23,\n 101,\n 102,\n 15,\n 120,\n 121,\n 122,\n 123,\n 23,\n 94,\n 15,\n 124,\n 23,\n 125,\n 69,\n 126,\n 127,\n 8,\n 23,\n 117,\n 128,\n 129,\n 1,\n 130,\n 131,\n 132,\n 133,\n 134,\n 123,\n 135,\n 136,\n 15,\n 137,\n 23,\n 138,\n 139,\n 69,\n 140,\n 126,\n 127,\n 8,\n 23,\n 117,\n 141,\n 69,\n 23,\n 98,\n 142,\n 143,\n 137,\n 144,\n 145,\n 146,\n 118,\n 113,\n 114,\n 115,\n 116,\n 8,\n 147,\n 143,\n 148,\n 149,\n 150,\n 8,\n 136,\n 15,\n 22,\n 151,\n 8,\n 152,\n 153,\n 154,\n 155,\n 8,\n 156,\n 157,\n 158,\n 22,\n 159,\n 142,\n 15,\n 160,\n 161,\n 162,\n 163,\n 123,\n 23,\n 164,\n 165,\n 69,\n 149,\n 150,\n 8,\n 23,\n 166,\n 167,\n 82,\n 168,\n 169,\n 136,\n 170,\n 171,\n 132,\n 147,\n 143,\n 15,\n 172,\n 132,\n 173,\n 23,\n 142,\n 69,\n 174,\n 118,\n 175,\n 176,\n 177,\n 178,\n 15,\n 178,\n 72,\n 179,\n 137,\n 129,\n 1,\n 180,\n 181,\n 182,\n 183,\n 184,\n 185,\n 96,\n 186,\n 56,\n 128,\n 187,\n 188,\n 189,\n 190,\n 191,\n 192,\n 193,\n 194,\n 195,\n 196,\n 197,\n 15,\n 198,\n 199,\n 200,\n 201,\n 202,\n 154,\n 203,\n 8,\n 204,\n 15,\n 22,\n 205,\n 140,\n 133,\n 200,\n 206,\n 207,\n 208,\n 123,\n 209,\n 8,\n 210,\n 141,\n 134,\n 123,\n 211,\n 212,\n 172,\n 213,\n 154,\n 23,\n 214,\n 15,\n 215,\n 72,\n 100,\n 216,\n 217,\n 218,\n 219,\n 148,\n 220,\n 221,\n 222,\n 8,\n 136,\n 47,\n 223,\n 224,\n 15,\n 225,\n 11,\n 226,\n 227,\n 167,\n 228,\n 229,\n 8,\n 136,\n 15,\n 23,\n 117,\n 200,\n 230,\n 231,\n 225,\n 11,\n 232,\n 115,\n 116,\n 233,\n 23,\n 129,\n 234,\n 153,\n 235,\n 236,\n 237,\n 23,\n 200,\n 152,\n 153,\n 15,\n 23,\n 117,\n 200,\n 238,\n 239,\n 143,\n 240,\n 123,\n 149,\n 150,\n 15,\n 93,\n 94,\n 15,\n 116,\n 223,\n 241,\n 8,\n 126,\n 127,\n 8,\n 115,\n 116,\n 2,\n 242,\n 159,\n 243,\n 244,\n 149,\n 150,\n 123,\n 245,\n 23,\n 129,\n 234,\n 132,\n 246,\n 147,\n 143,\n 148,\n 90,\n 92,\n 15,\n 159,\n 247,\n 95,\n 38,\n 248,\n 249,\n 250,\n 251,\n 252,\n 121,\n 123,\n 253,\n 100,\n 254,\n 255,\n 256,\n 257,\n 258,\n 259,\n 260,\n 47,\n 23,\n 129,\n 234,\n 128,\n 261,\n 262,\n 244,\n 199,\n 263,\n 15,\n 23,\n 264,\n 69,\n 115,\n 116,\n 91,\n 200,\n 265,\n 261,\n 244,\n 23,\n 266,\n 15,\n 116,\n 223,\n 267,\n 8,\n 251,\n 252,\n 121,\n 91,\n 254,\n 255,\n 256,\n 257,\n 258,\n 259,\n 260,\n 268,\n 23,\n 129,\n 1,\n 141,\n 15,\n 103,\n 269,\n 23,\n 117,\n 141,\n 12,\n 270,\n 271,\n 15,\n 23,\n 272,\n 130,\n 131,\n 273,\n 274,\n 197,\n 275,\n 22,\n 23,\n 276,\n 128,\n 277,\n 278,\n 90,\n 91,\n 92,\n 15,\n 279,\n 280,\n 281,\n 69,\n 282,\n 128,\n 283,\n 284,\n 285,\n 286,\n 47,\n 133,\n 287,\n 8,\n 23,\n 117,\n 200,\n 288,\n 289,\n 15,\n 251,\n 290,\n 291,\n 121,\n 91,\n 261,\n 153,\n 148,\n 104,\n 292,\n 293,\n 294,\n 8,\n 231,\n 295,\n 296,\n 232,\n 152,\n 297,\n 298,\n 23,\n 129,\n 1,\n 299,\n 300,\n 301,\n 302,\n 38,\n 303,\n 243,\n 128,\n 269,\n 262,\n 123,\n 304,\n 305,\n 306,\n 307,\n 308,\n 309,\n 310,\n 15,\n 311,\n 312,\n 23,\n 129,\n 234,\n 313,\n 23,\n 117,\n 314,\n 128,\n 315,\n 123,\n 316,\n 317,\n 318,\n 319,\n 312,\n 15,\n 320,\n 23,\n 94,\n 8,\n 115,\n 116,\n 72,\n 119,\n 128,\n 174,\n 321,\n 322,\n 323,\n 15,\n 175,\n 324,\n 325,\n 326,\n 327,\n 328,\n 15,\n 84,\n 329,\n 8,\n 251,\n 252,\n 121,\n 91,\n 330,\n 153,\n 331,\n 328,\n 23,\n 271,\n 128,\n 159,\n 200,\n 132,\n 332,\n 143,\n 15,\n 279,\n 23,\n 333,\n 261,\n 148,\n 154,\n 334,\n 87,\n 157,\n 335,\n 8,\n 23,\n 129,\n 234,\n 288,\n 289,\n 15,\n 336,\n 159,\n 335,\n 337,\n 338,\n 339,\n 340,\n 128,\n 159,\n 341,\n 342,\n 343,\n 152,\n 288,\n 289,\n 123,\n 149,\n 150,\n 15,\n 305,\n 344,\n 345,\n 346,\n 123,\n 347,\n 8,\n 23,\n 129,\n 1,\n 141,\n 348,\n 349,\n 350,\n 351,\n 15,\n 352,\n 353,\n 8,\n 103,\n 72,\n 100,\n 354,\n 38,\n 23,\n 129,\n 1,\n 141,\n 355,\n 23,\n 117,\n 141,\n 356,\n 23,\n 357,\n 15,\n 358,\n 8,\n 103,\n 83,\n 359,\n 360,\n 100,\n 354,\n 38,\n 23,\n 117,\n 355,\n 172,\n 361,\n 362,\n 135,\n 69,\n 136,\n 15,\n 23,\n 129,\n 1,\n 141,\n 95,\n 23,\n 94,\n 69,\n 90,\n 363,\n 82,\n 92,\n 8,\n 23,\n 94,\n 69,\n 364,\n 186,\n 365,\n 366,\n 367,\n 82,\n 10,\n 368,\n 15,\n 369,\n 370,\n 69,\n 371,\n 372,\n 373,\n 374,\n 15,\n 375,\n 117,\n 158,\n 376,\n 23,\n 377,\n 69,\n 371,\n 38,\n 199,\n 378,\n 22,\n 23,\n 94,\n 15,\n 172,\n 132,\n 87,\n 379,\n 380,\n 144,\n 381,\n 93,\n 382,\n 144,\n 381,\n 72,\n 100,\n 144,\n 381,\n 22,\n 23,\n 117,\n 141,\n 320,\n 23,\n 13,\n 101,\n 102,\n 15,\n 103,\n 383,\n 384,\n 385,\n 386,\n 384,\n 158,\n 387,\n 244,\n 23,\n 388,\n 69,\n 382,\n 8,\n 389,\n 15,\n 103,\n 72,\n 390,\n 69,\n 23,\n 141,\n 69,\n 23,\n 391,\n 41,\n 392,\n 15,\n 393,\n 394,\n 395,\n 396,\n 218,\n 290,\n 393,\n 394,\n 397,\n 395,\n 396,\n 218,\n 290,\n 4,\n 218,\n 398,\n 203,\n 8,\n 399,\n 400,\n 401,\n 267,\n 8,\n 402,\n 10,\n 72,\n 12,\n 13,\n 403,\n 8,\n 14,\n 128,\n 404,\n 194,\n 259,\n 405,\n 15,\n 16,\n 45,\n 46,\n 406,\n 407,\n 69,\n 408,\n 137,\n 409,\n 394,\n 410,\n 259,\n 411,\n 412,\n 8,\n 413,\n 414,\n 415,\n 416,\n 250,\n 128,\n 417,\n 111,\n 418,\n 419,\n 420,\n 15,\n 16,\n 421,\n 12,\n 422,\n 423,\n 8,\n 25,\n 424,\n 425,\n 99,\n 22,\n 401,\n 426,\n 15,\n 427,\n 428,\n 429,\n 25,\n 430,\n 431,\n 43,\n 72,\n 421,\n 22,\n 432,\n 15,\n 100,\n 138,\n 433,\n 8,\n 25,\n 434,\n 100,\n 99,\n 72,\n 421,\n 22,\n 402,\n 15,\n 435,\n 15,\n 393,\n 394,\n 397,\n 395,\n 396,\n 218,\n 290,\n 72,\n 73,\n 128,\n 436,\n 22,\n 76,\n 437,\n 15,\n 438,\n 438,\n 439,\n 15,\n 440,\n 396,\n 218,\n 290,\n 72,\n 441,\n 137,\n 7,\n 167,\n 442,\n 167,\n 443,\n 444,\n 445,\n 446,\n 418]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_subset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#implementing the same but via nltk MLE\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m testing_subset_str \u001B[38;5;241m=\u001B[39m \u001B[43mtesting_subset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtolist\u001B[49m()\n\u001B[1;32m      4\u001B[0m testing_subset_str \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mstr\u001B[39m(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m testing_subset]\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(testing_subset_str[:\u001B[38;5;241m10\u001B[39m])\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'list' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "#implementing the same but via nltk MLE\n",
    "\n",
    "testing_subset_str = testing_subset.tolist()\n",
    "testing_subset_str = [str(i) for i in testing_subset]\n",
    "\n",
    "print(testing_subset_str[:10])\n",
    "\n",
    "train_bigram_lm = MLE(2)\n",
    "train_bigram_lm.fit(bigrams(testing_subset_str), vocabulary_text=meta_vocab_size)\n",
    "train_bigram_lm.vocab\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<zip object at 0x7fee51a343c0>, <zip object at 0x7fee51a3bc00>]\n",
      "[<zip object at 0x7fee51a3bc80>, <zip object at 0x7fee51a3b900>]\n",
      "<NgramCounter with 2 ngram orders and 14 ngrams>\n"
     ]
    }
   ],
   "source": [
    "text = [[\"a\", \"b\", \"c\", \"d\"], [\"a\", \"c\", \"d\", \"c\"]]\n",
    "from nltk.util import ngrams\n",
    "text_bigrams = [ngrams(sent, 2) for sent in text]\n",
    "text_unigrams = [ngrams(sent, 1) for sent in text]\n",
    "\n",
    "print(list(text_bigrams))\n",
    "print(list(text_unigrams))\n",
    "\n",
    "from nltk.lm import NgramCounter\n",
    "ngram_counts = NgramCounter(text_bigrams + text_unigrams)\n",
    "print(ngram_counts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testing_subset_str' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 6\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#train, vocab = padded_everygram_pipeline(2, testing_subset_str)\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvocabulary\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Vocabulary\n\u001B[0;32m----> 6\u001B[0m lm \u001B[38;5;241m=\u001B[39m Laplace(\u001B[38;5;241m2\u001B[39m, vocabulary \u001B[38;5;241m=\u001B[39m Vocabulary(\u001B[43mtesting_subset_str\u001B[49m, unk_cutoff\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)) \u001B[38;5;66;03m#, counter=Counter(bigrams(testing_subset_str)))\u001B[39;00m\n\u001B[1;32m      7\u001B[0m lm\u001B[38;5;241m.\u001B[39mfit(bigrams(testing_subset_str))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'testing_subset_str' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "#train, vocab = padded_everygram_pipeline(2, testing_subset_str)\n",
    "from nltk.lm.vocabulary import Vocabulary\n",
    "\n",
    "\n",
    "lm = Laplace(2, vocabulary = Vocabulary(testing_subset_str, unk_cutoff=1)) #, counter=Counter(bigrams(testing_subset_str)))\n",
    "lm.fit(bigrams(testing_subset_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Counter' object has no attribute 'unigrams'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[138], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mlm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscore\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/masters_thesis/mt1/lib/python3.9/site-packages/nltk/lm/api.py:124\u001B[0m, in \u001B[0;36mLanguageModel.score\u001B[0;34m(self, word, context)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mscore\u001B[39m(\u001B[38;5;28mself\u001B[39m, word, context\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    119\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Masks out of vocab (OOV) words and computes their model score.\u001B[39;00m\n\u001B[1;32m    120\u001B[0m \n\u001B[1;32m    121\u001B[0m \u001B[38;5;124;03m    For model-specific logic of calculating scores, see the `unmasked_score`\u001B[39;00m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;124;03m    method.\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 124\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munmasked_score\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    125\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlookup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlookup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[1;32m    126\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/masters_thesis/mt1/lib/python3.9/site-packages/nltk/lm/models.py:47\u001B[0m, in \u001B[0;36mLidstone.unmasked_score\u001B[0;34m(self, word, context)\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munmasked_score\u001B[39m(\u001B[38;5;28mself\u001B[39m, word, context\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     42\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Add-one smoothing: Lidstone or Laplace.\u001B[39;00m\n\u001B[1;32m     43\u001B[0m \n\u001B[1;32m     44\u001B[0m \u001B[38;5;124;03m    To see what kind, look at `gamma` attribute on the class.\u001B[39;00m\n\u001B[1;32m     45\u001B[0m \n\u001B[1;32m     46\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 47\u001B[0m     counts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontext_counts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m     word_count \u001B[38;5;241m=\u001B[39m counts[word]\n\u001B[1;32m     49\u001B[0m     norm_count \u001B[38;5;241m=\u001B[39m counts\u001B[38;5;241m.\u001B[39mN()\n",
      "File \u001B[0;32m~/PycharmProjects/masters_thesis/mt1/lib/python3.9/site-packages/nltk/lm/api.py:160\u001B[0m, in \u001B[0;36mLanguageModel.context_counts\u001B[0;34m(self, context)\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcontext_counts\u001B[39m(\u001B[38;5;28mself\u001B[39m, context):\n\u001B[1;32m    153\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Helper method for retrieving counts for a given context.\u001B[39;00m\n\u001B[1;32m    154\u001B[0m \n\u001B[1;32m    155\u001B[0m \u001B[38;5;124;03m    Assumes context has been checked and oov words in it masked.\u001B[39;00m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;124;03m    :type context: tuple(str) or None\u001B[39;00m\n\u001B[1;32m    157\u001B[0m \n\u001B[1;32m    158\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    159\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 160\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcounts[\u001B[38;5;28mlen\u001B[39m(context) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m][context] \u001B[38;5;28;01mif\u001B[39;00m context \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcounts\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munigrams\u001B[49m\n\u001B[1;32m    161\u001B[0m     )\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Counter' object has no attribute 'unigrams'"
     ]
    }
   ],
   "source": [
    "lm.score('2')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[135], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i,val \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mlist\u001B[39m(bigrams(testing_subset_str))):\n\u001B[0;32m----> 2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(val, \u001B[43mlm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscore\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m, ts_bigram_probs[itos_mapper[\u001B[38;5;28mint\u001B[39m(val[\u001B[38;5;241m1\u001B[39m])], itos_mapper[\u001B[38;5;28mint\u001B[39m(val[\u001B[38;5;241m0\u001B[39m])]])\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m10\u001B[39m:\n\u001B[1;32m      4\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/masters_thesis/mt1/lib/python3.9/site-packages/nltk/lm/api.py:124\u001B[0m, in \u001B[0;36mLanguageModel.score\u001B[0;34m(self, word, context)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mscore\u001B[39m(\u001B[38;5;28mself\u001B[39m, word, context\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    119\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Masks out of vocab (OOV) words and computes their model score.\u001B[39;00m\n\u001B[1;32m    120\u001B[0m \n\u001B[1;32m    121\u001B[0m \u001B[38;5;124;03m    For model-specific logic of calculating scores, see the `unmasked_score`\u001B[39;00m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;124;03m    method.\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 124\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munmasked_score\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    125\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlookup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlookup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[1;32m    126\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/masters_thesis/mt1/lib/python3.9/site-packages/nltk/lm/models.py:47\u001B[0m, in \u001B[0;36mLidstone.unmasked_score\u001B[0;34m(self, word, context)\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munmasked_score\u001B[39m(\u001B[38;5;28mself\u001B[39m, word, context\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     42\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Add-one smoothing: Lidstone or Laplace.\u001B[39;00m\n\u001B[1;32m     43\u001B[0m \n\u001B[1;32m     44\u001B[0m \u001B[38;5;124;03m    To see what kind, look at `gamma` attribute on the class.\u001B[39;00m\n\u001B[1;32m     45\u001B[0m \n\u001B[1;32m     46\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 47\u001B[0m     counts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontext_counts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m     word_count \u001B[38;5;241m=\u001B[39m counts[word]\n\u001B[1;32m     49\u001B[0m     norm_count \u001B[38;5;241m=\u001B[39m counts\u001B[38;5;241m.\u001B[39mN()\n",
      "File \u001B[0;32m~/PycharmProjects/masters_thesis/mt1/lib/python3.9/site-packages/nltk/lm/api.py:160\u001B[0m, in \u001B[0;36mLanguageModel.context_counts\u001B[0;34m(self, context)\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcontext_counts\u001B[39m(\u001B[38;5;28mself\u001B[39m, context):\n\u001B[1;32m    153\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Helper method for retrieving counts for a given context.\u001B[39;00m\n\u001B[1;32m    154\u001B[0m \n\u001B[1;32m    155\u001B[0m \u001B[38;5;124;03m    Assumes context has been checked and oov words in it masked.\u001B[39;00m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;124;03m    :type context: tuple(str) or None\u001B[39;00m\n\u001B[1;32m    157\u001B[0m \n\u001B[1;32m    158\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    159\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 160\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcounts\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m context \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcounts\u001B[38;5;241m.\u001B[39munigrams\n\u001B[1;32m    161\u001B[0m     )\n",
      "\u001B[0;31mTypeError\u001B[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for i,val in enumerate(list(bigrams(testing_subset_str))):\n",
    "    print(val, lm.score(val[1], val[0]), ts_bigram_probs[itos_mapper[int(val[1])], itos_mapper[int(val[0])]])\n",
    "    if i > 10:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "3.726726889641174"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.entropy(list(bigrams(testing_subset_str)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14545184/14545184 [02:47<00:00, 87028.08it/s]\n",
      "100%|██████████| 13921893/13921893 [02:37<00:00, 88392.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train bigram loss = 5.702608585357666\n",
      "Val bigram loss = 5.8382182121276855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Loss calculator =\n",
    "\n",
    "\n",
    "train_bigram_loss = calculate_loss(train_bigram_probs, train_data, 'bigram')\n",
    "val_bigram_loss = calculate_loss(train_bigram_probs, val_data, 'bigram')\n",
    "\n",
    "\n",
    "print(f\"Train bigram loss = {train_bigram_loss}\")\n",
    "print(f\"Val bigram loss = {val_bigram_loss}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testing_subset ="
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 18540.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(8.1286)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_bigram_loss(train_probs, [1,2,3,4,5,6,7,8,9,10])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6434288/6434288 [00:07<00:00, 908132.70it/s] \n",
      "100%|██████████| 6026069/6026069 [00:06<00:00, 900709.44it/s] \n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 370, 565) 0.00024838549428713363\n",
      "(370, 565, 39) 0.00024996875390576176\n",
      "(565, 39, 391) 0.00024909702329057166\n",
      "(39, 391, 1753) 0.0045871559633027525\n",
      "(391, 1753, 2315) 0.007099603257465024\n",
      "(1753, 2315, 3465) 0.0004958472790380563\n",
      "(2315, 3465, 12) 0.0016223636590540372\n",
      "(3465, 12, 6812) 0.0003638568829593693\n",
      "(12, 6812, 9) 0.001120099564405725\n",
      "(6812, 9, 229) 0.00224271118863693\n",
      "(9, 229, 267) 0.07475936828333801\n"
     ]
    }
   ],
   "source": [
    "for i, (k,v) in enumerate(train_trigram_probs.items()):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(k, v)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14545183/14545183 [00:26<00:00, 546067.32it/s]\n",
      "100%|██████████| 13921892/13921892 [00:25<00:00, 546449.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trigram loss = 6.911967814899822\n",
      "Val trigram loss = 7.324238471596719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Loss calculator =\n",
    "\n",
    "\n",
    "train_trigram_loss = calculate_trigram_loss(train_trigram_probs, train_data)\n",
    "val_trigram_loss = calculate_trigram_loss(train_trigram_probs, val_data)\n",
    "\n",
    "print(f\"Train trigram loss = {train_trigram_loss}\")\n",
    "print(f\"Val trigram loss = {val_trigram_loss}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't choose from empty population",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#print(lm.perplexity(val))\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mlm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_seed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_seed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m<s>\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/PycharmProjects/masters_thesis/mt1/lib/python3.9/site-packages/nltk/lm/api.py:229\u001B[0m, in \u001B[0;36mLanguageModel.generate\u001B[0;34m(self, num_words, text_seed, random_seed)\u001B[0m\n\u001B[1;32m    226\u001B[0m generated \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_words):\n\u001B[1;32m    228\u001B[0m     generated\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m--> 229\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    230\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnum_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtext_seed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_seed\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mgenerated\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrandom_seed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    233\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    234\u001B[0m     )\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m generated\n",
      "File \u001B[0;32m~/PycharmProjects/masters_thesis/mt1/lib/python3.9/site-packages/nltk/lm/api.py:220\u001B[0m, in \u001B[0;36mLanguageModel.generate\u001B[0;34m(self, num_words, text_seed, random_seed)\u001B[0m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# Sorting samples achieves two things:\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m# - reproducible randomness when sampling\u001B[39;00m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m# - turns Mapping into Sequence which `_weighted_choice` expects\u001B[39;00m\n\u001B[1;32m    219\u001B[0m     samples \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(samples)\n\u001B[0;32m--> 220\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_weighted_choice\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[43m        \u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    222\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscore\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msamples\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    224\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;66;03m# We build up text one word at a time using the preceding context.\u001B[39;00m\n\u001B[1;32m    226\u001B[0m generated \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/PycharmProjects/masters_thesis/mt1/lib/python3.9/site-packages/nltk/lm/api.py:64\u001B[0m, in \u001B[0;36m_weighted_choice\u001B[0;34m(population, weights, random_generator)\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Like random.choice, but with weights.\u001B[39;00m\n\u001B[1;32m     60\u001B[0m \n\u001B[1;32m     61\u001B[0m \u001B[38;5;124;03mHeavily inspired by python 3.6 `random.choices`.\u001B[39;00m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m population:\n\u001B[0;32m---> 64\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt choose from empty population\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(population) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(weights):\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe number of weights does not match the population\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Can't choose from empty population"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
