{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-09T01:39:19.334727Z",
     "start_time": "2024-09-09T01:39:19.332450Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T02:26:31.720625Z",
     "start_time": "2024-09-09T02:26:31.680457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logs_folder = '/home/abishekthamma/PycharmProjects/masters_thesis/ss-llm/logs'\n",
    "run_ids = [7761393, 7761405, 7761425, 7761428, 7761445]\n",
    "\n",
    "def return_val_given_line_str(line_str, split_char=\"=\"):\n",
    "    #usually line of the form \"x = y #comment\"\n",
    "    return_val = line_str.split(split_char)[1].split(\"#\")[0].strip().replace(\",\", \"\")\n",
    "    if \"*\" in return_val:\n",
    "        return_val = eval(return_val)\n",
    "    \n",
    "    return return_val\n",
    "\n",
    "def return_iter_vals(line_str):\n",
    "    #usually line of the form \"iter 150: loss 6.9318, time 313.88ms, mfu 38.50%\n",
    "    #print(line_str)\n",
    "    iter_num = int(line_str.split(\":\")[0].split(\" \")[1])\n",
    "    loss = float(line_str.split(\":\")[1].split(\",\")[0].split(\" \")[2])\n",
    "    time = line_str.split(\":\")[1].split(\",\")[1].split(\" \")[2].strip()\n",
    "    mfu = line_str.split(\":\")[1].split(\",\")[2].split(\" \")[2].strip()\n",
    "    \n",
    "    return iter_num, loss, time, mfu\n",
    "\n",
    "def return_step_vals(line_str):\n",
    "    #step 250: train loss 6.4297, val loss 6.4395\n",
    "    step_num = int(line_str.split(\":\")[0].split(\" \")[1])\n",
    "    train_loss = float(line_str.split(\":\")[1].split(\",\")[0].strip().split(\" \")[2])\n",
    "    val_loss = float(line_str.split(\":\")[1].split(\",\")[1].strip().split(\" \")[2])\n",
    "    \n",
    "    return step_num, train_loss, val_loss\n",
    "\n",
    "def  stats_from_log(log_run_id):\n",
    "    params_dict = {}\n",
    "    with open(os.path.join(logs_folder, f'{log_run_id}.out'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if line[0] == \"#\":\n",
    "                continue\n",
    "            if \"batch_size\" in line:\n",
    "                if \"batch_size\" not in params_dict:\n",
    "                    params_dict['batch_size'] = int(return_val_given_line_str(line))\n",
    "            \n",
    "            if \"gradient_accumulation_steps\" in line:\n",
    "                if \"gradient_accumulation_steps\" not in params_dict:\n",
    "                    params_dict['gradient_accumulation_steps'] = int(return_val_given_line_str(line))\n",
    "            \n",
    "            if \"block_size\" in line:\n",
    "                if \"block_size\" not in params_dict:\n",
    "                    params_dict['block_size'] = int(return_val_given_line_str(line))\n",
    "            \n",
    "            if \"tokens per iteration will be\" in line:\n",
    "                if \"tokens_per_iteration\" not in params_dict:\n",
    "                    tokens_per_iteration = int(return_val_given_line_str(line, split_char=\":\"))\n",
    "                    assert params_dict['block_size']*params_dict['batch_size']*params_dict['gradient_accumulation_steps'] == tokens_per_iteration, \"Tokens per iteration mismatch\"\n",
    "                    params_dict['tokens_per_iteration'] = tokens_per_iteration\n",
    "                    \n",
    "            if \"iter 150\" in line:\n",
    "                iter_num, loss, time, mfu = return_iter_vals(line)\n",
    "                if \"iterations\" not in params_dict:\n",
    "                    params_dict['iteration_num'] = iter_num\n",
    "                if \"loss\" not in params_dict:\n",
    "                    params_dict[f'train_loss_{iter_num}'] = loss\n",
    "                if \"time\" not in params_dict:\n",
    "                    params_dict[f'train_time_{iter_num}'] = time\n",
    "                if \"mfu\" not in params_dict:\n",
    "                    params_dict[f'train_mfu_{iter_num}'] = mfu\n",
    "            \n",
    "            if \"iter 325\" in line:\n",
    "                iter_num, loss, time, mfu = return_iter_vals(line)\n",
    "                if \"iterations\" not in params_dict:\n",
    "                    params_dict['iteration_num'] = iter_num\n",
    "                if \"loss\" not in params_dict:\n",
    "                    params_dict[f'train_loss_{iter_num}'] = loss\n",
    "                if \"time\" not in params_dict:\n",
    "                    params_dict[f'train_time_{iter_num}'] = time\n",
    "                if \"mfu\" not in params_dict:\n",
    "                    params_dict[f'train_mfu_{iter_num}'] = mfu\n",
    "            \n",
    "            if \"step 250\" in line:\n",
    "                step_num, train_loss, val_loss = return_step_vals(line)\n",
    "                if \"steps\" not in params_dict:\n",
    "                    params_dict['step_num'] = step_num\n",
    "                if \"train_loss\" not in params_dict:\n",
    "                    params_dict[f'train_loss_{step_num}'] = train_loss\n",
    "                if \"val_loss\" not in params_dict:\n",
    "                    params_dict[f'val_loss_{step_num}'] = val_loss\n",
    "            \n",
    "            if \"step 500\" in line:\n",
    "                step_num, train_loss, val_loss = return_step_vals(line)\n",
    "                if \"steps\" not in params_dict:\n",
    "                    params_dict['step_num'] = step_num\n",
    "                if \"train_loss\" not in params_dict:\n",
    "                    params_dict[f'train_loss_{step_num}'] = train_loss\n",
    "                if \"val_loss\" not in params_dict:\n",
    "                    params_dict[f'val_loss_{step_num}'] = val_loss\n",
    "                \n",
    "            if \"CPU Utilized\" in line:\n",
    "                if \"cpu_util\" not in params_dict:\n",
    "                    params_dict['cpu_util'] = \":\".join(line.split(\":\")[1:]).strip()\n",
    "            \n",
    "            if \"CPU Efficiency\" in line:\n",
    "                if \"cpu_efficiency\" not in params_dict:\n",
    "                    params_dict['cpu_efficiency'] = \":\".join(line.split(\":\")[1:]).strip()\n",
    "            \n",
    "            if \"Memory Utilized\" in line:\n",
    "                if \"memory_util\" not in params_dict:\n",
    "                    params_dict['memory_util'] = \":\".join(line.split(\":\")[1:]).strip()\n",
    "            \n",
    "            if \"Memory Efficiency\" in line:\n",
    "                if \"memory_efficiency\" not in params_dict:\n",
    "                    params_dict['memory_efficiency'] = line.split(\":\")[1].strip()\n",
    "                    \n",
    "            if \"Job Wall-clock time\" in line:\n",
    "                if \"wall_clock_time\" not in params_dict:\n",
    "                    params_dict['wall_clock_time'] = \":\".join(line.split(\":\")[1:]).strip()\n",
    "            \n",
    "            \n",
    "    return params_dict\n",
    "\n",
    "stats_logs = []\n",
    "for run_id in run_ids:\n",
    "    stats_logs.append(stats_from_log(run_id))\n",
    "\n",
    "pd.DataFrame(stats_logs)\n",
    "            \n",
    "            \n",
    "        "
   ],
   "id": "36a81b40d0dd1082",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   gradient_accumulation_steps  batch_size  block_size  tokens_per_iteration  \\\n",
       "0                            8          32         256                 65536   \n",
       "1                            4          64         256                 65536   \n",
       "2                            8          64         256                131072   \n",
       "3                           16          64         256                262144   \n",
       "4                            4         128         256                131072   \n",
       "\n",
       "   iteration_num  train_loss_150 train_time_150 train_mfu_150  step_num  \\\n",
       "0            150          6.9708        89.76ms        33.65%       500   \n",
       "1            150          6.9671        83.44ms        35.36%       500   \n",
       "2            150          6.9517       160.46ms        37.68%       500   \n",
       "3            150          6.9318       313.88ms        38.50%       500   \n",
       "4            150          6.9378       152.62ms        38.79%       500   \n",
       "\n",
       "   train_loss_250  val_loss_250  train_loss_500  val_loss_500  cpu_util  \\\n",
       "0          6.5243        6.5198          6.2471        6.2645  00:12:10   \n",
       "1          6.5345        6.5344          6.2585        6.2703  00:12:58   \n",
       "2          6.4695        6.4844          6.1989        6.1946  00:15:31   \n",
       "3          6.4297        6.4395          6.1489        6.1576  00:00:00   \n",
       "4          6.4711        6.4797          6.2000        6.2094  00:17:25   \n",
       "\n",
       "                    cpu_efficiency wall_clock_time  \\\n",
       "0  5.79% of 03:30:00 core-walltime        00:02:55   \n",
       "1  6.07% of 03:33:36 core-walltime        00:02:58   \n",
       "2  5.90% of 04:22:48 core-walltime        00:03:39   \n",
       "3  0.00% of 06:02:24 core-walltime        00:05:02   \n",
       "4  5.97% of 04:51:36 core-walltime        00:04:03   \n",
       "\n",
       "                   memory_util                    memory_efficiency  \n",
       "0                     10.96 GB                   2.28% of 480.00 GB  \n",
       "1                     10.79 GB                   2.25% of 480.00 GB  \n",
       "2                     10.82 GB                   2.25% of 480.00 GB  \n",
       "3  0.00 MB (estimated maximum)  0.00% of 480.00 GB (480.00 GB/node)  \n",
       "4                     10.90 GB                   2.27% of 480.00 GB  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gradient_accumulation_steps</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>block_size</th>\n",
       "      <th>tokens_per_iteration</th>\n",
       "      <th>iteration_num</th>\n",
       "      <th>train_loss_150</th>\n",
       "      <th>train_time_150</th>\n",
       "      <th>train_mfu_150</th>\n",
       "      <th>step_num</th>\n",
       "      <th>train_loss_250</th>\n",
       "      <th>val_loss_250</th>\n",
       "      <th>train_loss_500</th>\n",
       "      <th>val_loss_500</th>\n",
       "      <th>cpu_util</th>\n",
       "      <th>cpu_efficiency</th>\n",
       "      <th>wall_clock_time</th>\n",
       "      <th>memory_util</th>\n",
       "      <th>memory_efficiency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>256</td>\n",
       "      <td>65536</td>\n",
       "      <td>150</td>\n",
       "      <td>6.9708</td>\n",
       "      <td>89.76ms</td>\n",
       "      <td>33.65%</td>\n",
       "      <td>500</td>\n",
       "      <td>6.5243</td>\n",
       "      <td>6.5198</td>\n",
       "      <td>6.2471</td>\n",
       "      <td>6.2645</td>\n",
       "      <td>00:12:10</td>\n",
       "      <td>5.79% of 03:30:00 core-walltime</td>\n",
       "      <td>00:02:55</td>\n",
       "      <td>10.96 GB</td>\n",
       "      <td>2.28% of 480.00 GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>256</td>\n",
       "      <td>65536</td>\n",
       "      <td>150</td>\n",
       "      <td>6.9671</td>\n",
       "      <td>83.44ms</td>\n",
       "      <td>35.36%</td>\n",
       "      <td>500</td>\n",
       "      <td>6.5345</td>\n",
       "      <td>6.5344</td>\n",
       "      <td>6.2585</td>\n",
       "      <td>6.2703</td>\n",
       "      <td>00:12:58</td>\n",
       "      <td>6.07% of 03:33:36 core-walltime</td>\n",
       "      <td>00:02:58</td>\n",
       "      <td>10.79 GB</td>\n",
       "      <td>2.25% of 480.00 GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>256</td>\n",
       "      <td>131072</td>\n",
       "      <td>150</td>\n",
       "      <td>6.9517</td>\n",
       "      <td>160.46ms</td>\n",
       "      <td>37.68%</td>\n",
       "      <td>500</td>\n",
       "      <td>6.4695</td>\n",
       "      <td>6.4844</td>\n",
       "      <td>6.1989</td>\n",
       "      <td>6.1946</td>\n",
       "      <td>00:15:31</td>\n",
       "      <td>5.90% of 04:22:48 core-walltime</td>\n",
       "      <td>00:03:39</td>\n",
       "      <td>10.82 GB</td>\n",
       "      <td>2.25% of 480.00 GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>256</td>\n",
       "      <td>262144</td>\n",
       "      <td>150</td>\n",
       "      <td>6.9318</td>\n",
       "      <td>313.88ms</td>\n",
       "      <td>38.50%</td>\n",
       "      <td>500</td>\n",
       "      <td>6.4297</td>\n",
       "      <td>6.4395</td>\n",
       "      <td>6.1489</td>\n",
       "      <td>6.1576</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0.00% of 06:02:24 core-walltime</td>\n",
       "      <td>00:05:02</td>\n",
       "      <td>0.00 MB (estimated maximum)</td>\n",
       "      <td>0.00% of 480.00 GB (480.00 GB/node)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>131072</td>\n",
       "      <td>150</td>\n",
       "      <td>6.9378</td>\n",
       "      <td>152.62ms</td>\n",
       "      <td>38.79%</td>\n",
       "      <td>500</td>\n",
       "      <td>6.4711</td>\n",
       "      <td>6.4797</td>\n",
       "      <td>6.2000</td>\n",
       "      <td>6.2094</td>\n",
       "      <td>00:17:25</td>\n",
       "      <td>5.97% of 04:51:36 core-walltime</td>\n",
       "      <td>00:04:03</td>\n",
       "      <td>10.90 GB</td>\n",
       "      <td>2.27% of 480.00 GB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T02:38:01.366091Z",
     "start_time": "2024-09-09T02:37:57.721885Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e106e96028401ed6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53b3c7d96f4349b6bad7733374e2f7a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.38.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "49f5e08e3222acf0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
