{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from collections import OrderedDict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 2., 7., 7., 5.])\n",
      "tensor([0.1466, 0.0027, 0.3984, 0.3984, 0.0539])\n",
      "tensor(1.)\n",
      "tensor([0.0440, 0.0013, 0.2789, 0.3586, 0.0539])\n",
      "tensor(0.7367)\n",
      "tensor([0.1785, 0.1710, 0.2257, 0.2445, 0.1803])\n",
      "tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_594521/66765972.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_array = torch.tensor(torch.randint(1, 10, (5,)), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "#Create random array of 5 elements between 1 and 9\n",
    "\n",
    "test_array = torch.tensor(torch.randint(1, 10, (5,)), dtype=torch.float32)\n",
    "print(test_array)\n",
    "\n",
    "#sOFtmax function\n",
    "test_array_softmax = F.softmax(test_array, dim=0)\n",
    "\n",
    "print(test_array_softmax)\n",
    "\n",
    "print(sum(test_array_softmax))\n",
    "\n",
    "#Multiply softmax by exponential mask\n",
    "wm_mask = [0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "\n",
    "test_array_softmax_masked = test_array_softmax * torch.tensor(wm_mask, dtype=torch.float32)\n",
    "\n",
    "print(test_array_softmax_masked)\n",
    "print(sum(test_array_softmax_masked))\n",
    "\n",
    "softmaxing = F.softmax(test_array_softmax_masked, dim=0)\n",
    "\n",
    "print(softmaxing)\n",
    "print(sum(softmaxing))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def params(n_embd, block_size, vocab_size, n_layer, n_head, hs_dim = 1, ffw_size = 1):\n",
    "    \"\"\" estimates the number of parameters in the model\"\"\"\n",
    "    \"\"\" estimates the number of parameters in the model\"\"\"\n",
    "\n",
    "    if hs_dim == 1:\n",
    "        hs_dim = n_embd\n",
    "    out = OrderedDict()\n",
    "\n",
    "    # token and position embeddings\n",
    "    out['emebedding/position'] = n_embd * block_size\n",
    "    out['embedding/token'] = n_embd * vocab_size\n",
    "    out['embedding'] = out['emebedding/position'] + out['embedding/token']\n",
    "\n",
    "    # attention blocks\n",
    "    out['attention/ln'] = n_embd # note, bias=False in our LN\n",
    "    out['attention/kqv'] = n_embd * 3*hs_dim   #n_embd * 3*n_embd\n",
    "    out['attention/proj'] = hs_dim*n_embd #n_embd**2\n",
    "    out['attention'] = out['attention/ln'] + out['attention/kqv'] + out['attention/proj']\n",
    "\n",
    "    # MLP blocks\n",
    "    if ffw_size == 1:\n",
    "        ffw_size = 4*n_embd # feed forward size\n",
    "    #ffw_size = 4*n_embd # feed forward size\n",
    "    out['mlp/ln'] = n_embd\n",
    "    out['mlp/ffw'] = n_embd * ffw_size\n",
    "    out['mlp/proj'] = ffw_size * n_embd\n",
    "    out['mlp'] = out['mlp/ln'] + out['mlp/ffw'] + out['mlp/proj']\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out['block'] = out['attention'] + out['mlp']\n",
    "    out['transformer'] = n_layer * out['block']\n",
    "    out['ln_f'] = n_embd # final layernorm\n",
    "    out['dense'] = 0 # 0 because of parameter sharing. This layer uses the weights from the embedding layer\n",
    "\n",
    "    # total\n",
    "    out['total'] = out['embedding'] + out['transformer'] + out['ln_f'] + out['dense']\n",
    "\n",
    "    return out\n",
    "\n",
    "# compare our param count to that reported by PyTorch\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "ID                         2x12         4x12        6x12        12x12  \\\nparams               6614784.00  10155264.00  13695744.0  24317184.00   \nparams_m                   6.61        10.16        13.7        24.32   \nemebedding/position     1920.00      1920.00      1920.0      1920.00   \nembedding/token      3072000.00   3072000.00   3072000.0   3072000.00   \nembedding            3073920.00   3073920.00   3073920.0   3073920.00   \nattention/ln             384.00       384.00       384.0       384.00   \nattention/kqv         442368.00    442368.00    442368.0    442368.00   \nattention/proj        147456.00    147456.00    147456.0    147456.00   \nattention             590208.00    590208.00    590208.0    590208.00   \nmlp/ln                   384.00       384.00       384.0       384.00   \nmlp/ffw               589824.00    589824.00    589824.0    589824.00   \nmlp/proj              589824.00    589824.00    589824.0    589824.00   \nmlp                  1180032.00   1180032.00   1180032.0   1180032.00   \nblock                1770240.00   1770240.00   1770240.0   1770240.00   \ntransformer          3540480.00   7080960.00  10621440.0  21242880.00   \nln_f                     384.00       384.00       384.0       384.00   \ndense                      0.00         0.00         0.0         0.00   \ntotal                6614784.00  10155264.00  13695744.0  24317184.00   \nn_layer                    2.00         4.00         6.0        12.00   \n\nID                        16x12        24x12  \nparams               31398144.0  45560064.00  \nparams_m                   31.4        45.56  \nemebedding/position      1920.0      1920.00  \nembedding/token       3072000.0   3072000.00  \nembedding             3073920.0   3073920.00  \nattention/ln              384.0       384.00  \nattention/kqv          442368.0    442368.00  \nattention/proj         147456.0    147456.00  \nattention              590208.0    590208.00  \nmlp/ln                    384.0       384.00  \nmlp/ffw                589824.0    589824.00  \nmlp/proj               589824.0    589824.00  \nmlp                   1180032.0   1180032.00  \nblock                 1770240.0   1770240.00  \ntransformer          28323840.0  42485760.00  \nln_f                      384.0       384.00  \ndense                       0.0         0.00  \ntotal                31398144.0  45560064.00  \nn_layer                    16.0        24.00  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>ID</th>\n      <th>2x12</th>\n      <th>4x12</th>\n      <th>6x12</th>\n      <th>12x12</th>\n      <th>16x12</th>\n      <th>24x12</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>params</th>\n      <td>6614784.00</td>\n      <td>10155264.00</td>\n      <td>13695744.0</td>\n      <td>24317184.00</td>\n      <td>31398144.0</td>\n      <td>45560064.00</td>\n    </tr>\n    <tr>\n      <th>params_m</th>\n      <td>6.61</td>\n      <td>10.16</td>\n      <td>13.7</td>\n      <td>24.32</td>\n      <td>31.4</td>\n      <td>45.56</td>\n    </tr>\n    <tr>\n      <th>emebedding/position</th>\n      <td>1920.00</td>\n      <td>1920.00</td>\n      <td>1920.0</td>\n      <td>1920.00</td>\n      <td>1920.0</td>\n      <td>1920.00</td>\n    </tr>\n    <tr>\n      <th>embedding/token</th>\n      <td>3072000.00</td>\n      <td>3072000.00</td>\n      <td>3072000.0</td>\n      <td>3072000.00</td>\n      <td>3072000.0</td>\n      <td>3072000.00</td>\n    </tr>\n    <tr>\n      <th>embedding</th>\n      <td>3073920.00</td>\n      <td>3073920.00</td>\n      <td>3073920.0</td>\n      <td>3073920.00</td>\n      <td>3073920.0</td>\n      <td>3073920.00</td>\n    </tr>\n    <tr>\n      <th>attention/ln</th>\n      <td>384.00</td>\n      <td>384.00</td>\n      <td>384.0</td>\n      <td>384.00</td>\n      <td>384.0</td>\n      <td>384.00</td>\n    </tr>\n    <tr>\n      <th>attention/kqv</th>\n      <td>442368.00</td>\n      <td>442368.00</td>\n      <td>442368.0</td>\n      <td>442368.00</td>\n      <td>442368.0</td>\n      <td>442368.00</td>\n    </tr>\n    <tr>\n      <th>attention/proj</th>\n      <td>147456.00</td>\n      <td>147456.00</td>\n      <td>147456.0</td>\n      <td>147456.00</td>\n      <td>147456.0</td>\n      <td>147456.00</td>\n    </tr>\n    <tr>\n      <th>attention</th>\n      <td>590208.00</td>\n      <td>590208.00</td>\n      <td>590208.0</td>\n      <td>590208.00</td>\n      <td>590208.0</td>\n      <td>590208.00</td>\n    </tr>\n    <tr>\n      <th>mlp/ln</th>\n      <td>384.00</td>\n      <td>384.00</td>\n      <td>384.0</td>\n      <td>384.00</td>\n      <td>384.0</td>\n      <td>384.00</td>\n    </tr>\n    <tr>\n      <th>mlp/ffw</th>\n      <td>589824.00</td>\n      <td>589824.00</td>\n      <td>589824.0</td>\n      <td>589824.00</td>\n      <td>589824.0</td>\n      <td>589824.00</td>\n    </tr>\n    <tr>\n      <th>mlp/proj</th>\n      <td>589824.00</td>\n      <td>589824.00</td>\n      <td>589824.0</td>\n      <td>589824.00</td>\n      <td>589824.0</td>\n      <td>589824.00</td>\n    </tr>\n    <tr>\n      <th>mlp</th>\n      <td>1180032.00</td>\n      <td>1180032.00</td>\n      <td>1180032.0</td>\n      <td>1180032.00</td>\n      <td>1180032.0</td>\n      <td>1180032.00</td>\n    </tr>\n    <tr>\n      <th>block</th>\n      <td>1770240.00</td>\n      <td>1770240.00</td>\n      <td>1770240.0</td>\n      <td>1770240.00</td>\n      <td>1770240.0</td>\n      <td>1770240.00</td>\n    </tr>\n    <tr>\n      <th>transformer</th>\n      <td>3540480.00</td>\n      <td>7080960.00</td>\n      <td>10621440.0</td>\n      <td>21242880.00</td>\n      <td>28323840.0</td>\n      <td>42485760.00</td>\n    </tr>\n    <tr>\n      <th>ln_f</th>\n      <td>384.00</td>\n      <td>384.00</td>\n      <td>384.0</td>\n      <td>384.00</td>\n      <td>384.0</td>\n      <td>384.00</td>\n    </tr>\n    <tr>\n      <th>dense</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>total</th>\n      <td>6614784.00</td>\n      <td>10155264.00</td>\n      <td>13695744.0</td>\n      <td>24317184.00</td>\n      <td>31398144.0</td>\n      <td>45560064.00</td>\n    </tr>\n    <tr>\n      <th>n_layer</th>\n      <td>2.00</td>\n      <td>4.00</td>\n      <td>6.0</td>\n      <td>12.00</td>\n      <td>16.0</td>\n      <td>24.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "settings = [\n",
    "    {\"block_size\" : 5, \"vocab_size\" : 8000, \"n_layer\" : 2, \"n_head\" : 12, \"n_embd\" : 384, \"hs_dim\": 1},\n",
    "    {\"block_size\" : 5, \"vocab_size\" : 8000, \"n_layer\" : 4, \"n_head\" : 12, \"n_embd\" : 384, \"hs_dim\": 1},\n",
    "    {\"block_size\" : 5, \"vocab_size\" : 8000, \"n_layer\" : 6, \"n_head\" : 12, \"n_embd\" : 384, \"hs_dim\":1},\n",
    "    {\"block_size\" : 5, \"vocab_size\" : 8000, \"n_layer\" : 12, \"n_head\" : 12, \"n_embd\" : 384, \"hs_dim\":1},\n",
    "    {\"block_size\" : 5, \"vocab_size\" : 8000, \"n_layer\" : 16, \"n_head\" : 12, \"n_embd\" : 384, \"hs_dim\": 1},\n",
    "    {\"block_size\" : 5, \"vocab_size\" : 8000, \"n_layer\" : 24, \"n_head\" : 12, \"n_embd\" : 384},\n",
    "    #{\"block_size\" : 1024, \"vocab_size\" : 50257, \"n_layer\" : 12, \"n_head\" : 12, \"n_embd\" : 768},  # 124M params\n",
    "    #{\"block_size\" : 1024, \"vocab_size\" : 50257, \"n_layer\" : 24, \"n_head\" : 16, \"n_embd\" : 1024}, # 350M params\n",
    "    #{\"block_size\" : 1024, \"vocab_size\" : 50257, \"n_layer\" : 36, \"n_head\" : 20, \"n_embd\" : 1280},\n",
    "    #{\"block_size\" : 1024, \"vocab_size\" : 50257, \"n_layer\" : 48, \"n_head\" : 25, \"n_embd\" : 1600},\n",
    "\n",
    "]\n",
    "\n",
    "#p = params(**settings[0])\n",
    "#params_total = p['total']\n",
    "\n",
    "#print(f\"we see: {params_total/1e6:.2f}M parameters\")\n",
    "# create a header\n",
    "#print(f\"{'name':20s} {'params':10s} {'ratio (%)':10s}\")\n",
    "#for k,v in p.items():\n",
    "#    print(f\"{k:20s} {v:10d} {v/params_total*100:10.4f}\")\n",
    "\n",
    "for i in range(len(settings)):\n",
    "    settings_tmp = {}#settings[i]\n",
    "\n",
    "    p = params(**settings[i])\n",
    "    params_total = p['total']\n",
    "    settings_tmp[\"params\"] = params_total\n",
    "    settings_tmp[\"params_m\"] = round(params_total/1e6, 2)\n",
    "    for k,v in p.items():\n",
    "        #if k in [\"attention\", \"mlp\", \"transformer\"]:\n",
    "        settings_tmp[k] = v\n",
    "    settings_tmp[\"ID\"] = str(settings[i][\"n_layer\"])+\"x\"+str(settings[i][\"n_head\"])\n",
    "    settings_tmp[\"n_layer\"] = settings[i][\"n_layer\"]\n",
    "    #results.append(p)\n",
    "    #print(f\"we see: {params_total/1e6:.2f}M parameters\")\n",
    "    #print(f\"{'name':20s} {'params':10s} {'ratio (%)':10s}\")\n",
    "    #for k,v in p.items():\n",
    "    #    print(f\"{k:20s} {v:10d} {v/params_total*100:10.4f}\")\n",
    "\n",
    "    results.append(settings_tmp)\n",
    "\n",
    "#transpose_columns = [k for k in results[0].keys() if k not in settings[0].keys()]\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.set_index(\"ID\", inplace=True)\n",
    "#df = df.transpose()\n",
    "#df[\"attention_total\"] = df[\"attention\"] * df[\"n_layer\"]\n",
    "#df[\"mlp_total\"] = df[\"mlp\"] * df[\"n_layer\"]\n",
    "#df[\"param_minus_embedding\"] = df[\"params\"] - df[\"embedding\"]\n",
    "#df[\"tot_sum\"] = df[\"attention_total\"] + df[\"mlp_total\"]\n",
    "\n",
    "\n",
    "\n",
    "df.T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "       params_m  attention      mlp  transformer  n_layer  attention_total  \\\nID                                                                           \n2x12      24.31    9437568  1180032     21235200        2         18875136   \n4x12      24.31    4129152  1180032     21236736        4         16516608   \n6x12      24.31    2359680  1180032     21238272        6         14158080   \n12x12     24.32     590208  1180032     21242880       12          7082496   \n16x12     24.32     147840  1180032     21245952       16          2365440   \n24x12     45.56     590208  1180032     42485760       24         14164992   \n\n       mlp_total   tot_sum  \nID                          \n2x12     2360064  21235200  \n4x12     4720128  21236736  \n6x12     7080192  21238272  \n12x12   14160384  21242880  \n16x12   18880512  21245952  \n24x12   28320768  42485760  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>params_m</th>\n      <th>attention</th>\n      <th>mlp</th>\n      <th>transformer</th>\n      <th>n_layer</th>\n      <th>attention_total</th>\n      <th>mlp_total</th>\n      <th>tot_sum</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2x12</th>\n      <td>24.31</td>\n      <td>9437568</td>\n      <td>1180032</td>\n      <td>21235200</td>\n      <td>2</td>\n      <td>18875136</td>\n      <td>2360064</td>\n      <td>21235200</td>\n    </tr>\n    <tr>\n      <th>4x12</th>\n      <td>24.31</td>\n      <td>4129152</td>\n      <td>1180032</td>\n      <td>21236736</td>\n      <td>4</td>\n      <td>16516608</td>\n      <td>4720128</td>\n      <td>21236736</td>\n    </tr>\n    <tr>\n      <th>6x12</th>\n      <td>24.31</td>\n      <td>2359680</td>\n      <td>1180032</td>\n      <td>21238272</td>\n      <td>6</td>\n      <td>14158080</td>\n      <td>7080192</td>\n      <td>21238272</td>\n    </tr>\n    <tr>\n      <th>12x12</th>\n      <td>24.32</td>\n      <td>590208</td>\n      <td>1180032</td>\n      <td>21242880</td>\n      <td>12</td>\n      <td>7082496</td>\n      <td>14160384</td>\n      <td>21242880</td>\n    </tr>\n    <tr>\n      <th>16x12</th>\n      <td>24.32</td>\n      <td>147840</td>\n      <td>1180032</td>\n      <td>21245952</td>\n      <td>16</td>\n      <td>2365440</td>\n      <td>18880512</td>\n      <td>21245952</td>\n    </tr>\n    <tr>\n      <th>24x12</th>\n      <td>45.56</td>\n      <td>590208</td>\n      <td>1180032</td>\n      <td>42485760</td>\n      <td>24</td>\n      <td>14164992</td>\n      <td>28320768</td>\n      <td>42485760</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "settings = [\n",
    "    {\"block_size\" : 5, \"vocab_size\" : 8000, \"n_layer\" : 2, \"n_head\" : 12, \"n_embd\" : 384, \"hs_dim\": 6144},\n",
    "    {\"block_size\" : 5, \"vocab_size\" : 8000, \"n_layer\" : 4, \"n_head\" : 12, \"n_embd\" : 384, \"hs_dim\": 2688},\n",
    "    {\"block_size\" : 5, \"vocab_size\" : 8000, \"n_layer\" : 6, \"n_head\" : 12, \"n_embd\" : 384, \"hs_dim\": 1536},\n",
    "    {\"block_size\" : 5, \"vocab_size\" : 8000, \"n_layer\" : 12, \"n_head\" : 12, \"n_embd\" : 384, \"hs_dim\":384},\n",
    "    {\"block_size\" : 5, \"vocab_size\" : 8000, \"n_layer\" : 16, \"n_head\" : 12, \"n_embd\" : 384, \"hs_dim\": 96},\n",
    "    {\"block_size\" : 5, \"vocab_size\" : 8000, \"n_layer\" : 24, \"n_head\" : 12, \"n_embd\" : 384, \"hs_dim\": 1},\n",
    "]\n",
    "\n",
    "#p = params(**settings[0])\n",
    "#params_total = p['total']\n",
    "\n",
    "#print(f\"we see: {params_total/1e6:.2f}M parameters\")\n",
    "# create a header\n",
    "#print(f\"{'name':20s} {'params':10s} {'ratio (%)':10s}\")\n",
    "#for k,v in p.items():\n",
    "#    print(f\"{k:20s} {v:10d} {v/params_total*100:10.4f}\")\n",
    "\n",
    "for i in range(len(settings)):\n",
    "    settings_tmp = {}#settings[i]\n",
    "\n",
    "    p = params(**settings[i])\n",
    "    params_total = p['total']\n",
    "    #settings_tmp[\"params\"] = params_total\n",
    "    settings_tmp[\"params_m\"] = round(params_total/1e6, 2)\n",
    "    for k,v in p.items():\n",
    "        if k in [\"attention\", \"mlp\", \"transformer\"]:\n",
    "            settings_tmp[k] = v\n",
    "    settings_tmp[\"ID\"] = str(settings[i][\"n_layer\"])+\"x\"+str(settings[i][\"n_head\"])\n",
    "    settings_tmp[\"n_layer\"] = settings[i][\"n_layer\"]\n",
    "    #results.append(p)\n",
    "    #print(f\"we see: {params_total/1e6:.2f}M parameters\")\n",
    "    #print(f\"{'name':20s} {'params':10s} {'ratio (%)':10s}\")\n",
    "    #for k,v in p.items():\n",
    "    #    print(f\"{k:20s} {v:10d} {v/params_total*100:10.4f}\")\n",
    "\n",
    "    results.append(settings_tmp)\n",
    "\n",
    "#transpose_columns = [k for k in results[0].keys() if k not in settings[0].keys()]\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.set_index(\"ID\", inplace=True)\n",
    "#df = df.transpose()\n",
    "df[\"attention_total\"] = df[\"attention\"] * df[\"n_layer\"]\n",
    "df[\"mlp_total\"] = df[\"mlp\"] * df[\"n_layer\"]\n",
    "#df[\"param_minus_embedding\"] = df[\"params\"] - df[\"embedding\"]\n",
    "df[\"tot_sum\"] = df[\"attention_total\"] + df[\"mlp_total\"]\n",
    "\n",
    "\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "         params  params_m  attention       mlp     block  transformer  \\\nID                                                                      \n2x12   24309504     24.31     590208  10027392  10617600     21235200   \n4x12   24311040     24.31     590208   4718976   5309184     21236736   \n6x12   24312576     24.31     590208   2949504   3539712     21238272   \n12x12  24317184     24.32     590208   1180032   1770240     21242880   \n16x12  24320256     24.32     590208    737664   1327872     21245952   \n24x12  24326400     24.33     590208    295296    885504     21252096   \n\n       n_layer  attention_total  mlp_total   tot_sum  A_totminusffb  \nID                                                                   \n2x12         2          1180416   20054784  21235200         590208  \n4x12         4          2360832   18875904  21236736         590208  \n6x12         6          3541248   17697024  21238272         590208  \n12x12       12          7082496   14160384  21242880         590208  \n16x12       16          9443328   11802624  21245952         590208  \n24x12       24         14164992    7087104  21252096         590208  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>params</th>\n      <th>params_m</th>\n      <th>attention</th>\n      <th>mlp</th>\n      <th>block</th>\n      <th>transformer</th>\n      <th>n_layer</th>\n      <th>attention_total</th>\n      <th>mlp_total</th>\n      <th>tot_sum</th>\n      <th>A_totminusffb</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2x12</th>\n      <td>24309504</td>\n      <td>24.31</td>\n      <td>590208</td>\n      <td>10027392</td>\n      <td>10617600</td>\n      <td>21235200</td>\n      <td>2</td>\n      <td>1180416</td>\n      <td>20054784</td>\n      <td>21235200</td>\n      <td>590208</td>\n    </tr>\n    <tr>\n      <th>4x12</th>\n      <td>24311040</td>\n      <td>24.31</td>\n      <td>590208</td>\n      <td>4718976</td>\n      <td>5309184</td>\n      <td>21236736</td>\n      <td>4</td>\n      <td>2360832</td>\n      <td>18875904</td>\n      <td>21236736</td>\n      <td>590208</td>\n    </tr>\n    <tr>\n      <th>6x12</th>\n      <td>24312576</td>\n      <td>24.31</td>\n      <td>590208</td>\n      <td>2949504</td>\n      <td>3539712</td>\n      <td>21238272</td>\n      <td>6</td>\n      <td>3541248</td>\n      <td>17697024</td>\n      <td>21238272</td>\n      <td>590208</td>\n    </tr>\n    <tr>\n      <th>12x12</th>\n      <td>24317184</td>\n      <td>24.32</td>\n      <td>590208</td>\n      <td>1180032</td>\n      <td>1770240</td>\n      <td>21242880</td>\n      <td>12</td>\n      <td>7082496</td>\n      <td>14160384</td>\n      <td>21242880</td>\n      <td>590208</td>\n    </tr>\n    <tr>\n      <th>16x12</th>\n      <td>24320256</td>\n      <td>24.32</td>\n      <td>590208</td>\n      <td>737664</td>\n      <td>1327872</td>\n      <td>21245952</td>\n      <td>16</td>\n      <td>9443328</td>\n      <td>11802624</td>\n      <td>21245952</td>\n      <td>590208</td>\n    </tr>\n    <tr>\n      <th>24x12</th>\n      <td>24326400</td>\n      <td>24.33</td>\n      <td>590208</td>\n      <td>295296</td>\n      <td>885504</td>\n      <td>21252096</td>\n      <td>24</td>\n      <td>14164992</td>\n      <td>7087104</td>\n      <td>21252096</td>\n      <td>590208</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "settings = [\n",
    "    {\"block_size\" : 5, \"vocab_size\" : 8000, \"n_layer\" : 2, \"n_head\" : 12, \"n_embd\" : 384, \"hs_dim\": 384, \"ffw_size\" : 13056},\n",
    "    {\"block_size\" : 5, \"vocab_size\" : 8000, \"n_layer\" : 4, \"n_head\" : 12, \"n_embd\" : 384, \"hs_dim\": 384, \"ffw_size\" : 6144},\n",
    "    {\"block_size\" : 5, \"vocab_size\" : 8000, \"n_layer\" : 6, \"n_head\" : 12, \"n_embd\" : 384, \"hs_dim\": 384, \"ffw_size\" : 3840},\n",
    "    {\"block_size\" : 5, \"vocab_size\" : 8000, \"n_layer\" : 12, \"n_head\" : 12, \"n_embd\" : 384, \"hs_dim\":384, \"ffw_size\" : 1},\n",
    "    {\"block_size\" : 5, \"vocab_size\" : 8000, \"n_layer\" : 16, \"n_head\" : 12, \"n_embd\" : 384, \"hs_dim\": 384, \"ffw_size\" : 960},\n",
    "    {\"block_size\" : 5, \"vocab_size\" : 8000, \"n_layer\" : 24, \"n_head\" : 12, \"n_embd\" : 384, \"hs_dim\": 1, \"ffw_size\" : 384},\n",
    "]\n",
    "\n",
    "#p = params(**settings[0])\n",
    "#params_total = p['total']\n",
    "\n",
    "#print(f\"we see: {params_total/1e6:.2f}M parameters\")\n",
    "# create a header\n",
    "#print(f\"{'name':20s} {'params':10s} {'ratio (%)':10s}\")\n",
    "#for k,v in p.items():\n",
    "#    print(f\"{k:20s} {v:10d} {v/params_total*100:10.4f}\")\n",
    "\n",
    "for i in range(len(settings)):\n",
    "    settings_tmp = {}#settings[i]\n",
    "\n",
    "    p = params(**settings[i])\n",
    "    params_total = p['total']\n",
    "    settings_tmp[\"params\"] = params_total\n",
    "    settings_tmp[\"params_m\"] = round(params_total/1e6, 2)\n",
    "    for k,v in p.items():\n",
    "        if k in [\"attention\", \"mlp\", \"transformer\", \"block\"]:\n",
    "            settings_tmp[k] = v\n",
    "    settings_tmp[\"ID\"] = str(settings[i][\"n_layer\"])+\"x\"+str(settings[i][\"n_head\"])\n",
    "    settings_tmp[\"n_layer\"] = settings[i][\"n_layer\"]\n",
    "    #results.append(p)\n",
    "    #print(f\"we see: {params_total/1e6:.2f}M parameters\")\n",
    "    #print(f\"{'name':20s} {'params':10s} {'ratio (%)':10s}\")\n",
    "    #for k,v in p.items():\n",
    "    #    print(f\"{k:20s} {v:10d} {v/params_total*100:10.4f}\")\n",
    "\n",
    "    results.append(settings_tmp)\n",
    "\n",
    "#transpose_columns = [k for k in results[0].keys() if k not in settings[0].keys()]\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.set_index(\"ID\", inplace=True)\n",
    "#df = df.transpose()\n",
    "df[\"attention_total\"] = df[\"attention\"] * df[\"n_layer\"]\n",
    "df[\"mlp_total\"] = df[\"mlp\"] * df[\"n_layer\"]\n",
    "#df[\"param_minus_embedding\"] = df[\"params\"] - df[\"embedding\"]\n",
    "df[\"tot_sum\"] = df[\"attention_total\"] + df[\"mlp_total\"]\n",
    "\n",
    "df[\"A_totminusffb\"] = df[\"block\"] - df[\"mlp\"]\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [
    {
     "data": {
      "text/plain": "        transformer       block  calc\nID                                   \n2x12   3.540480e+06   1770240.0   2.0\n4x12   7.080960e+06   1770240.0   4.0\n6x12   1.062144e+07   1770240.0   6.0\n12x12  8.495309e+07   7079424.0  12.0\n24x16  3.020390e+08  12584960.0  24.0\n36x20  7.078810e+08  19663360.0  36.0\n48x25  1.474714e+09  30723200.0  48.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>transformer</th>\n      <th>block</th>\n      <th>calc</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2x12</th>\n      <td>3.540480e+06</td>\n      <td>1770240.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4x12</th>\n      <td>7.080960e+06</td>\n      <td>1770240.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>6x12</th>\n      <td>1.062144e+07</td>\n      <td>1770240.0</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>12x12</th>\n      <td>8.495309e+07</td>\n      <td>7079424.0</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>24x16</th>\n      <td>3.020390e+08</td>\n      <td>12584960.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>36x20</th>\n      <td>7.078810e+08</td>\n      <td>19663360.0</td>\n      <td>36.0</td>\n    </tr>\n    <tr>\n      <th>48x25</th>\n      <td>1.474714e+09</td>\n      <td>30723200.0</td>\n      <td>48.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select the row with id \"params\"\n",
    "\n",
    "df_temp = df.loc[[\"transformer\", \"block\"]].T\n",
    "df_temp[\"calc\"] = df_temp[\"transformer\"] / df_temp[\"block\"]\n",
    "df_temp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for a single layer with n_embd = 384 is 1769472\n",
      "nl1 = 2\n",
      "wk value for k = -10 is -11520.0\n",
      "dff value for k = -10 is 13056.0\n",
      "nl1 = 4\n",
      "wk value for k = -8 is -4608.0\n",
      "dff value for k = -8 is 6144.0\n",
      "nl1 = 6\n",
      "wk value for k = -6 is -2304.0\n",
      "dff value for k = -6 is 3840.0\n",
      "nl1 = 16\n",
      "wk value for k = 4 is 576.0\n",
      "dff value for k = 4 is 960.0\n",
      "nl1 = 24\n",
      "wk value for k = 12 is 1152.0\n",
      "dff value for k = 12 is 384.0\n"
     ]
    }
   ],
   "source": [
    "n_embd = 384\n",
    "dff0 = 4*384 #Default value is 4*n_embd\n",
    "\n",
    "#Since for a single layer parameter calculation is\n",
    "# M = 2*n_embd*d_ff (mlp part) + 4*n_embd*n_embd (attention part, technically n_embd*d_attn but d_attn = n_embd)\n",
    "# Can be written as M = B*d_ff + A\n",
    "# Where A = 4*n_embd*n_embd and B = 2*n_embd\n",
    "\n",
    "print(f\"Parameters for a single layer with n_embd = {n_embd} is {2*n_embd*dff0 + 4*n_embd*n_embd}\")\n",
    "\n",
    "A = 4*384*384\n",
    "B = 2*384\n",
    "nl0 = 12 #number of layers for which you want to bring parameters count to\n",
    "for nl1 in [2,4,6,16,24]:\n",
    "    k = nl1-nl0\n",
    "    wk = ((1- (nl0/(nl0+k)) ) * (dff0 + (A/B)))\n",
    "    print(f\"nl1 = {nl1}\")\n",
    "    print(f\"wk value for k = {k} is {wk}\")\n",
    "    print(f\"dff value for k = {k} is {dff0-wk}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodel\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GPTConfig, GPT \n\u001B[1;32m      6\u001B[0m out_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      7\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'model'"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
