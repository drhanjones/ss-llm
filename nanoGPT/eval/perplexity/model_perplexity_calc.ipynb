{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T18:09:34.363369Z",
     "start_time": "2024-09-09T18:09:31.731724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import GPT2Tokenizer, AutoTokenizer\n",
    "import numpy as np"
   ],
   "id": "bfcb0b79574f482c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-09T18:09:34.373313Z",
     "start_time": "2024-09-09T18:09:34.368951Z"
    }
   },
   "source": [
    "model_name_list = ['out-babylm_full_bpe_8k-6x6-mask_log001-6617787',\n",
    " 'out-babylm_full_bpe_8k-6x6-nomask-curr_log-7047459_s42',\n",
    " 'out-babylm_full_bpe_8k-6x6-nomask-curr_log-7047460_s2347',\n",
    " 'out-babylm_full_bpe_8k-6x6-nomask-curr_log-7047461_s9',\n",
    " 'out-babylm_full_bpe_8k-6x6-nomask-curr_log-7047462_s616',\n",
    " 'out-babylm_full_bpe_8k-6x6-nomask-curr_log-7047464_s46674',\n",
    " 'out-babylm_full_bpe_8k-6x6-nomask-curr_log-7047466_s6747',\n",
    " 'out-babylm_full_bpe_8k-6x6-nomask-curr_log-7047467_s869',\n",
    " 'out-babylm_full_bpe_8k-6x6-nomask-curr_log-7047468_s466',\n",
    " 'out-babylm_full_bpe_8k-6x6-nomask-curr_log-7047469_s11111']\n",
    "\n",
    "tokenizers_root = r\"/home/abishekthamma/PycharmProjects/masters_thesis/ss-llm/nanoGPT/data\"\n",
    "out_root = r'/home/abishekthamma/PycharmProjects/masters_thesis/ss-llm/nanoGPT/output_dump'\n",
    "\n",
    "data_folder = r'babylm_full_bpe_8k'\n",
    "model_name = 'out-babylm_full_bpe_8k-6x6-mask_lin-5734459_s1337'\n",
    "\n",
    "#model_name = model_list[0]\n",
    "\n",
    "out_dir = os.path.join(out_root, model_name)\n",
    "data_dir = os.path.join(tokenizers_root, data_folder)\n",
    "\n",
    "device = \"cuda\""
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T18:11:01.125142Z",
     "start_time": "2024-09-09T18:10:59.644921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_model(out_dir, device):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained GPT model from a checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        out_dir (str): The directory where the checkpoint file is located.\n",
    "        device (torch.device): The device to load the model onto.\n",
    "\n",
    "    Returns:\n",
    "        GPT: The loaded GPT model.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the checkpoint file is not found.\n",
    "    \"\"\"\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    print(f\"Loading model from {ckpt_path}\")\n",
    "    #NANOGPT_ROOT = str(Path(__file__).parents[4])\n",
    "    NANOGPT_ROOT = r'/home/abishekthamma/PycharmProjects/masters_thesis/ss-llm/nanoGPT' #Edit later to be dynamic\n",
    "    sys.path.append(NANOGPT_ROOT)\n",
    "    from model import GPT, GPTConfig\n",
    "\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    # Backward compatibility for new model args for QKV and FFW Adjustments\n",
    "    if checkpoint[\"model_args\"].get(\"wm_decay_length\", None) is None:\n",
    "        #wm_decay_length = block_size\n",
    "        checkpoint[\"model_args\"][\"wm_decay_length\"] = checkpoint[\"model_args\"][\"block_size\"]\n",
    "    # Setting head size as 3 times n_embd if not set already\n",
    "    if checkpoint['model_args'].get('head_size_qkv', None) is None:\n",
    "        checkpoint['model_args']['head_size_qkv'] = checkpoint['model_args']['n_embd']\n",
    "\n",
    "    if checkpoint[\"model_args\"].get(\"ffw_dim\", None) is None:\n",
    "        checkpoint[\"model_args\"][\"ffw_dim\"] = 4 * checkpoint[\"model_args\"][\"n_embd\"]\n",
    "\n",
    "    #print(checkpoint['model_args'])\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "\n",
    "    load_model = GPT(gptconf)\n",
    "\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "    load_model.load_state_dict(state_dict)\n",
    "    load_model.eval()\n",
    "\n",
    "    load_model = load_model.to(device)\n",
    "\n",
    "    return load_model\n",
    "\n",
    "def load_tokenizer(data_dir):\n",
    "    \"\"\"\n",
    "    Load tokenizer for natural stories evaluation.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): The directory path where the tokenizer data is stored.\n",
    "\n",
    "    Returns:\n",
    "        tokenizer (Tokenizer): The loaded tokenizer object.\n",
    "\n",
    "    Raises:\n",
    "        NotImplementedError: If stoi/itos is not supported or found.\n",
    "\n",
    "    \"\"\"\n",
    "    meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "    load_meta = os.path.exists(meta_path)\n",
    "    if load_meta:\n",
    "        with open(meta_path, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "        if meta.get(\"custom_tokenizer\", False):\n",
    "            print(f\"Loading custom tokenizer from {data_dir}\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(data_dir, use_fast=False)\n",
    "        else:\n",
    "            if meta.get(\"stoi\", False):\n",
    "                raise NotImplementedError(\"stoi/itos not supported yet\")\n",
    "            else:\n",
    "                raise NotImplementedError(\"No stoi/itos found\")\n",
    "    else:\n",
    "        print(\"No meta.pkl found, using default GPT-2 tokenizer\")\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "    if not tokenizer.eos_token:\n",
    "        tokenizer.add_special_tokens({\"eos_token\": \"</s>\"})\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    tokenizer.padding_side = \"left\" #Add if needed?\n",
    "    return tokenizer\n",
    "\n",
    "def load_model_tokenizer(out_dir, data_dir, device):\n",
    "    model = load_model(out_dir, device)\n",
    "    tokenizer = load_tokenizer(data_dir)\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_dataset(data_dir, val_only=True):\n",
    "    \n",
    "    val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "    if val_only:\n",
    "        return None, val_data\n",
    "    else:\n",
    "        train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "        return train_data, val_data\n",
    "\n",
    "def get_perplexity(model, data, device=\"cuda\", stride=1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (GPT): The GPT model to evaluate.\n",
    "        data (np.memmap): The dataset to evaluate.\n",
    "        device (str): The device to run the model on.\n",
    "        stride (int): The stride to use when evaluating the model.\n",
    "\n",
    "    Returns:\n",
    "        float: The perplexity of the dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    nlls = [] #Negative log likelihoods\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data.shape[0] - 1, stride):\n",
    "            x = torch.tensor(data[i:i + stride]).to(device)\n",
    "            y = torch.tensor(data[i + 1:i + stride + 1]).to(device)\n",
    "            logits, _ = model(x)\n",
    "            nlls.append(torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), reduction='none').cpu().numpy())\n",
    "    nlls = np.concatenate(nlls)\n",
    "    return np.exp(np.mean(nlls))\n",
    "    \n",
    "\n",
    "    \n",
    "model, tokenizer = load_model_tokenizer(out_dir, data_dir, device)\n",
    "\n",
    "    \n"
   ],
   "id": "948e026a8ef86be8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /home/abishekthamma/PycharmProjects/masters_thesis/ss-llm/nanoGPT/output_dump/out-babylm_full_bpe_8k-6x6-mask_lin-5734459_s1337/ckpt.pt\n",
      "Setting flash to False because wm_mask is enabled\n",
      "Setting flash to False because wm_mask is enabled\n",
      "Setting flash to False because wm_mask is enabled\n",
      "Setting flash to False because wm_mask is enabled\n",
      "Setting flash to False because wm_mask is enabled\n",
      "Setting flash to False because wm_mask is enabled\n",
      "number of parameters: 13.69M\n",
      "Loading custom tokenizer from /home/abishekthamma/PycharmProjects/masters_thesis/ss-llm/nanoGPT/data/babylm_full_bpe_8k\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T19:15:33.470281Z",
     "start_time": "2024-09-09T19:15:31.951612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model, tokenizer = load_model_tokenizer(out_dir, data_dir, device)\n",
    "model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "type(model)\n"
   ],
   "id": "c32c75e3e617a0c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /home/abishekthamma/PycharmProjects/masters_thesis/ss-llm/nanoGPT/output_dump/out-babylm_full_bpe_8k-6x6-mask_lin-5734459_s1337/ckpt.pt\n",
      "Setting flash to False because wm_mask is enabled\n",
      "Setting flash to False because wm_mask is enabled\n",
      "Setting flash to False because wm_mask is enabled\n",
      "Setting flash to False because wm_mask is enabled\n",
      "Setting flash to False because wm_mask is enabled\n",
      "Setting flash to False because wm_mask is enabled\n",
      "number of parameters: 13.69M\n",
      "Loading custom tokenizer from /home/abishekthamma/PycharmProjects/masters_thesis/ss-llm/nanoGPT/data/babylm_full_bpe_8k\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T19:02:32.611186Z",
     "start_time": "2024-09-09T18:53:47.088662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "device = \"cuda\"\n",
    "model_id = \"openai-community/gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")#\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "stride = 64\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())\n",
    "print(ppl)\n"
   ],
   "id": "235049d9b19d396c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████▉| 4479/4495 [08:33<00:01,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.3362, device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
